# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11WjI5SCw9liTVG0ESQcdy5m56jFxqe4L
"""

# File path untuk dataset
file1 = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv"
file2 = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_SMOTE.csv"

# Import pustaka yang dibutuhkan
import pandas as pd

def describe_dataset(file_path):
    """
    Membaca dataset dan memberikan deskripsi detail.
    """
    try:
        # Membaca dataset
        data = pd.read_csv(file_path)

        # Menampilkan informasi dataset
        print(f"\n--- Deskripsi Dataset: {file_path} ---\n")
        print("1. Info Dataset:")
        print(data.info())  # Informasi tipe data dan jumlah data

        print("\n2. Statistik Deskriptif:")
        print(data.describe(include='all'))  # Statistik deskriptif

        print("\n3. Jumlah Nilai Kosong per Kolom:")
        print(data.isnull().sum())  # Jumlah nilai kosong

        print("\n4. Sampel Data:")
        print(data.head())  # Lima baris pertama dataset

    except Exception as e:
        print(f"Error membaca dataset {file_path}: {e}")

# Menjalankan fungsi untuk kedua file
describe_dataset(file1)
describe_dataset(file2)

# prompt: lihat total stroke = 1 dan stroke = 0 di dataset stroke_cleaned_for_model.csv

import pandas as pd

# File path untuk dataset
file1 = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv"

try:
    # Membaca dataset
    data = pd.read_csv(file1)

    # Menghitung jumlah stroke = 1 dan stroke = 0
    stroke_counts = data['stroke'].value_counts()
    print("\nJumlah Stroke:")
    print(stroke_counts)

except FileNotFoundError:
    print(f"Error: File '{file1}' tidak ditemukan.")
except KeyError:
    print(f"Error: Kolom 'stroke' tidak ditemukan dalam file '{file1}'.")
except Exception as e:
    print(f"Error membaca dataset: {e}")

# prompt: cek duplikasi stroke_cleaned_for_model.csv

import pandas as pd

# File path untuk dataset
file1 = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv"
file2 = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_SMOTE.csv"

def check_duplicates(file_path):
    """
    Memeriksa duplikat dalam dataset dan menampilkan jumlahnya.
    """
    try:
        data = pd.read_csv(file_path)
        duplicates = data[data.duplicated()]
        num_duplicates = len(duplicates)
        print(f"\n--- Duplikat dalam Dataset: {file_path} ---")
        print(f"Jumlah duplikat: {num_duplicates}")
        if num_duplicates > 0:
            print("\nData Duplikat:")
            print(duplicates)
    except Exception as e:
        print(f"Error membaca dataset {file_path}: {e}")

# Memeriksa duplikat pada kedua file
check_duplicates(file1)
check_duplicates(file2)

# prompt: hapus data duplikat

import pandas as pd

# File path untuk dataset
file1 = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv"
file2 = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_SMOTE.csv"

def remove_duplicates(file_path, output_file):
    """
    Menghapus duplikat dalam dataset dan menyimpannya ke file baru.
    """
    try:
        data = pd.read_csv(file_path)

        # Hapus duplikat, pertahankan yang pertama
        data_no_duplicates = data.drop_duplicates()

        # Simpan dataset tanpa duplikat ke file baru
        data_no_duplicates.to_csv(output_file, index=False)

        print(f"Data tanpa duplikat disimpan di: {output_file}")

    except Exception as e:
        print(f"Error memproses dataset {file_path}: {e}")

# Hapus duplikat dan simpan ke file baru
remove_duplicates(file1, "stroke_cleaned_for_model_no_duplicates.csv")
remove_duplicates(file2, "stroke_cleaned_SMOTE_no_duplicates.csv")

# prompt: download csv ke pc lokal

from google.colab import files
import pandas as pd

# File path untuk dataset
file1 = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv"
file2 = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_SMOTE.csv"

# Hapus duplikat dan simpan ke file baru (jika belum ada)
try:
    # Fungsi remove_duplicates dari kode sebelumnya
    def remove_duplicates(file_path, output_file):
        try:
            data = pd.read_csv(file_path)
            data_no_duplicates = data.drop_duplicates()
            data_no_duplicates.to_csv(output_file, index=False)
            print(f"Data tanpa duplikat disimpan di: {output_file}")
        except Exception as e:
            print(f"Error memproses dataset {file_path}: {e}")

    remove_duplicates(file1, "stroke_cleaned_for_model_no_duplicates.csv")
    remove_duplicates(file2, "stroke_cleaned_SMOTE_no_duplicates.csv")

    # Download file yang sudah dibersihkan duplikatnya
    files.download("stroke_cleaned_for_model_no_duplicates.csv")
    files.download("stroke_cleaned_SMOTE_no_duplicates.csv")

except FileNotFoundError:
    print("File tidak ditemukan. Pastikan path file sudah benar.")
except Exception as e:
    print(f"Terjadi kesalahan: {e}")

# File path untuk dataset
file1 = "stroke_cleaned_for_model_no_duplicates.csv"
file2 = "stroke_cleaned_SMOTE_no_duplicates.csv"

# Import pustaka yang dibutuhkan
import pandas as pd

def describe_dataset(file_path):
    """
    Membaca dataset dan memberikan deskripsi detail.
    """
    try:
        # Membaca dataset
        data = pd.read_csv(file_path)

        # Menampilkan informasi dataset
        print(f"\n--- Deskripsi Dataset: {file_path} ---\n")
        print("1. Info Dataset:")
        print(data.info())  # Informasi tipe data dan jumlah data

        print("\n2. Statistik Deskriptif:")
        print(data.describe(include='all'))  # Statistik deskriptif

        print("\n3. Jumlah Nilai Kosong per Kolom:")
        print(data.isnull().sum())  # Jumlah nilai kosong

        print("\n4. Sampel Data:")
        print(data.head())  # Lima baris pertama dataset

    except Exception as e:
        print(f"Error membaca dataset {file_path}: {e}")

# Menjalankan fungsi untuk kedua file
describe_dataset(file1)
describe_dataset(file2)

# Import library yang diperlukan
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# Membaca dataset
file_path = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv"
df = pd.read_csv(file_path)

# Menampilkan informasi dataset
print("Info Dataset:")
print(df.info())
print("\nStatistik Deskriptif:")
print(df.describe())

from pgmpy.estimators import PC
from pgmpy.models import BayesianNetwork

# Jalankan PC Algorithm menggunakan PGMPY
pc = PC(df)
dag = pc.estimate()

# Visualisasi hubungan sebab-akibat
print("Edges (Causal Relations):", dag.edges())

import matplotlib.pyplot as plt
import networkx as nx

# Definisikan edge dari hasil PC Algorithm
edges = [
    ('ever_married_yes', 'age'),
    ('ever_married_yes', 'work_type_children'),
    ('heart_disease', 'age'),
    ('stroke', 'age'),
    ('work_type_children', 'age'),
    ('avg_glucose_level', 'hypertension'),
    ('work_type_self_employed', 'work_type_private'),
    ('bmi', 'avg_glucose_level'),
    ('smoking_status_never_smoked', 'smoking_status_smokes'),
    ('smoking_status_formerly_smoked', 'smoking_status_smokes'),
    ('smoking_status_formerly_smoked', 'smoking_status_never_smoked')
]

# Membuat graf menggunakan NetworkX
G = nx.DiGraph()  # Directed Graph untuk hubungan kausal
G.add_edges_from(edges)

# Visualisasi graf
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G, seed=42)  # Layout graf untuk penempatan node
nx.draw(
    G, pos, with_labels=True, node_color='lightblue', edge_color='gray',
    node_size=3000, font_size=10, font_weight='bold', arrowsize=15
)

# Menambahkan label edge jika diperlukan
edge_labels = {edge: "" for edge in edges}
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9)

plt.title("Causal Graph from PC Algorithm", fontsize=14)
plt.show()

!pip install bnlearn

import pandas as pd
import bnlearn as bn

# Load dataset
file_path = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv"
df = pd.read_csv(file_path)

# Struktur pembelajaran menggunakan Hill-Climbing
model = bn.structure_learning.fit(df, methodtype='hc')  # Hill-Climbing algorithm

# Visualisasi graf kausal
bn.plot(model)

# Parameter pembelajaran
model = bn.parameter_learning.fit(model, df)

# Inferensi probabilitas stroke
q = bn.inference.fit(model, variables=['stroke'], evidence={'hypertension': 1, 'age': 60})
print(q)

import pandas as pd
import bnlearn as bn

# Load dataset
file_path = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv"
df = pd.read_csv(file_path)

# Struktur pembelajaran menggunakan PC algorithm
model_pc = bn.structure_learning.fit(df, methodtype='pc')  # PC algorithm

# Visualisasi graf kausal
bn.plot(model_pc)

# Parameter pembelajaran
model_pc = bn.parameter_learning.fit(model_pc, df)

# Inferensi probabilitas stroke
q = bn.inference.fit(model_pc, variables=['stroke'], evidence={'hypertension': 1, 'age': 60})
print(q)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
file_path = "/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv"
df = pd.read_csv(file_path)

# Fokus pada variabel terkait stroke
risk_factors = ["age", "heart_disease", "hypertension", "bmi", "avg_glucose_level",
                "smoking_status_formerly_smoked", "smoking_status_never_smoked",
                "smoking_status_smokes", "ever_married_yes"]
risk_factors.append("stroke")  # Tambahkan variabel target
df_risk = df[risk_factors]

# Korelasi variabel dengan stroke
corr = df_risk.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix: Stroke Risk Factors")
plt.show()

!pip install dowhy

from dowhy import CausalModel

# Contoh: Evaluasi efek hipertensi terhadap stroke
model = CausalModel(
    data=df_risk,
    treatment="hypertension",
    outcome="stroke",
    common_causes=["age", "bmi", "heart_disease", "avg_glucose_level"]
)

# Visualisasi model
model.view_model()

# Identifikasi estimand
identified_estimand = model.identify_effect()
print(identified_estimand)

# Estimasi efek kausal
estimate = model.estimate_effect(identified_estimand, method_name="backdoor.linear_regression")
print(f"Estimated Causal Effect of Hypertension on Stroke: {estimate.value}")

# Refutasi hasil estimasi
refutation = model.refute_estimate(identified_estimand, estimate, method_name="random_common_cause")
print(refutation)

# Distribusi variabel risiko berdasarkan stroke (0 = Tidak, 1 = Ya)
for var in risk_factors[:-1]:  # Kecuali "stroke"
    plt.figure(figsize=(8, 6))
    sns.boxplot(x="stroke", y=var, data=df_risk)
    plt.title(f"Distribution of {var} by Stroke Status")
    plt.xlabel("Stroke (0 = No, 1 = Yes)")
    plt.ylabel(var)
    plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score

# Dataset untuk prediksi
X = df_risk.drop(columns=["stroke"])
y = df_risk["stroke"]

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Random Forest Classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Evaluasi model
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print(f"ROC-AUC Score: {roc_auc_score(y_test, y_pred)}")

from google.colab import drive
drive.mount('/content/drive')

# Import pustaka yang diperlukan
import pandas as pd

# Load dataset
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv")

# Fungsi untuk menghitung P(stroke | feature)
def calculate_bayes(feature_name):
    # Prior probability P(stroke)
    p_stroke = data['stroke'].mean()

    # Probabilitas bersyarat P(feature | stroke) dan P(feature | not stroke)
    p_feature_given_stroke = data[data['stroke'] == 1][feature_name].mean()
    p_feature_given_not_stroke = data[data['stroke'] == 0][feature_name].mean()

    # Evidence P(feature)
    p_feature = data[feature_name].mean()

    # Hindari pembagian dengan nol
    if p_feature == 0:
        return None  # Mengembalikan None jika evidence nol

    # Bayesian probability P(stroke | feature)
    p_stroke_given_feature = (p_feature_given_stroke * p_stroke) / p_feature
    return p_stroke_given_feature

# Fitur-fitur yang ingin dihitung
features = ["age", "avg_glucose_level", "hypertension", "heart_disease", "ever_married_yes"]

# Hitung P(stroke | feature) untuk setiap fitur
results = {feature: calculate_bayes(feature) for feature in features}

# Tampilkan hasil
for feature, probability in results.items():
    print(f"P(stroke | {feature}) = {probability}")

import pandas as pd

# Load dataset
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv")

# Fungsi untuk menghitung P(stroke | feature) dengan dukungan variabel kategori
def calculate_bayes(feature_name):
    # Prior probability P(stroke)
    p_stroke = data['stroke'].mean()

    # Evidence P(feature)
    p_feature = data[feature_name].mean() if data[feature_name].dtype != 'object' else (
        data[feature_name].value_counts(normalize=True)
    )

    # Probabilitas bersyarat P(feature | stroke)
    if data[feature_name].dtype != 'object':  # Numerik
        p_feature_given_stroke = data[data['stroke'] == 1][feature_name].mean()
        p_feature_given_not_stroke = data[data['stroke'] == 0][feature_name].mean()
    else:  # Kategori
        p_feature_given_stroke = data[data['stroke'] == 1][feature_name].value_counts(normalize=True)
        p_feature_given_not_stroke = data[data['stroke'] == 0][feature_name].value_counts(normalize=True)

    # Hindari pembagian dengan nol
    if p_feature is None or p_feature == 0:
        return None

    # Bayesian probability P(stroke | feature)
    p_stroke_given_feature = (p_feature_given_stroke * p_stroke) / p_feature
    return p_stroke_given_feature

# Fitur-fitur yang ingin dihitung
features = ["age", "avg_glucose_level", "hypertension", "heart_disease", "ever_married_yes"]

# Hitung P(stroke | feature) untuk setiap fitur
results = {feature: calculate_bayes(feature) for feature in features}

# Tampilkan hasil
for feature, probability in results.items():
    print(f"P(stroke | {feature}) = {probability}")

import seaborn as sns
import matplotlib.pyplot as plt

# Contoh visualisasi untuk 'heart_disease'
sns.barplot(x='heart_disease', y='stroke', data=data)
plt.title('Probabilitas Stroke Berdasarkan Heart Disease')
plt.show()

!pip install pgmpy

# Import library
import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import HillClimbSearch, BicScore
from pgmpy.inference import VariableElimination
from sklearn.model_selection import train_test_split

# Load dataset
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv")

# Fokus pada fitur yang relevan
selected_features = ["age", "avg_glucose_level", "hypertension", "heart_disease", "ever_married_yes", "stroke"]
data = data[selected_features]

# Imputasi nilai hilang (jika ada)
data.fillna(data.median(), inplace=True)

# Diskritisasi data numerik (Bayesian Network membutuhkan data diskret)
for col in ["age", "avg_glucose_level"]:
    data[col] = pd.qcut(data[col], q=3, labels=False)

# Split dataset
train, test = train_test_split(data, test_size=0.2, random_state=42)

# 1. Struktur Bayesian Network menggunakan HillClimbSearch
hc = HillClimbSearch(train)
model_structure = hc.estimate(scoring_method=BicScore(train))

print("Edges in Bayesian Network:", model_structure.edges())

# 2. Definisikan model Bayesian Network berdasarkan struktur yang dipelajari
bn_model = BayesianNetwork(model_structure)

# 3. Fit model dengan data
bn_model.fit(train)

# 4. Inferensi menggunakan Variable Elimination
inference = VariableElimination(bn_model)

# Marginal Probabilities untuk target `stroke`
stroke_marginals = inference.query(variables=["stroke"])
print("\nMarginal probabilities for 'stroke':")
print(stroke_marginals)

# 5. Intervensi (Misalnya: hypertension = 1)
intervention_result = inference.query(variables=["stroke"], evidence={"hypertension": 1})
print("\nIntervention: P(stroke | hypertension=1):")
print(intervention_result)

# Intervensi pada heart_disease
intervention_hd = inference.query(variables=["stroke"], evidence={"heart_disease": 1})
print("\nIntervention: P(stroke | heart_disease=1):")
print(intervention_hd)

import matplotlib.pyplot as plt
import networkx as nx

# Konversi struktur Bayesian Network ke graf NetworkX
graph = nx.DiGraph(model_structure.edges())

# Visualisasi graf menggunakan NetworkX
plt.figure(figsize=(10, 7))
pos = nx.spring_layout(graph, seed=42)  # Layout graf
nx.draw(graph, pos, with_labels=True, node_size=3000, node_color='lightblue', font_size=10, font_weight='bold', arrowsize=20)
plt.title("Bayesian Network Structure")
plt.show()

# Intervensi pada 'age' (misalnya: kategori tertinggi age=2)
intervention_age = inference.query(variables=["stroke"], evidence={"age": 2})
print("\nIntervention: P(stroke | age=2):")
print(intervention_age)

# Intervensi pada 'hypertension' (misalnya: hypertension=1)
intervention_hypertension = inference.query(variables=["stroke"], evidence={"hypertension": 1})
print("\nIntervention: P(stroke | hypertension=1):")
print(intervention_hypertension)

# Intervensi pada 'heart_disease' (misalnya: heart_disease=1)
intervention_heart_disease = inference.query(variables=["stroke"], evidence={"heart_disease": 1})
print("\nIntervention: P(stroke | heart_disease=1):")
print(intervention_heart_disease)

import matplotlib.pyplot as plt
import numpy as np

# Data hasil intervensi
intervention_results = {
    "age=2": [0.8832, 0.1168],  # P(stroke=0), P(stroke=1)
    "hypertension=1": [0.9140, 0.0860],
    "heart_disease=1": [0.8984, 0.1016],
}

# Probabilitas tanpa intervensi (baseline)
baseline = [0.9542, 0.0458]  # P(stroke=0), P(stroke=1)

# Label variabel intervensi
labels = list(intervention_results.keys())

# Probabilitas stroke=1 untuk setiap intervensi
prob_stroke_1 = [intervention_results[intervention][1] for intervention in labels]

# Grafik batang
x = np.arange(len(labels))  # Posisi untuk label intervensi
width = 0.4

plt.figure(figsize=(10, 6))

# Plot baseline (tanpa intervensi)
plt.bar(x - width / 2, [baseline[1]] * len(labels), width, label="Baseline (P(stroke=1))", color="gray")

# Plot hasil intervensi
plt.bar(x + width / 2, prob_stroke_1, width, label="After Intervention (P(stroke=1))", color="skyblue")

# Tambahkan label dan judul
plt.ylabel("Probability of Stroke (P(stroke=1))")
plt.xlabel("Intervention")
plt.title("Effect of Interventions on Stroke Probability")
plt.xticks(x, labels)
plt.legend()

# Tampilkan grafik
plt.tight_layout()
plt.show()

# Kombinasi intervensi: age=2 dan heart_disease=1
intervention_combination = inference.query(
    variables=["stroke"],
    evidence={"age": 2, "heart_disease": 1}
)
print("\nIntervention: P(stroke | age=2, heart_disease=1):")
print(intervention_combination)

import matplotlib.pyplot as plt
import numpy as np

# Data hasil intervensi
baseline = [0.9542, 0.0458]  # P(stroke=0), P(stroke=1)
intervention_results = {
    "age=2": [0.8832, 0.1168],
    "heart_disease=1": [0.8984, 0.1016],
    "age=2 + heart_disease=1": [0.8832, 0.1168],  # Kombinasi intervensi
}

# Label intervensi
labels = list(intervention_results.keys())

# Probabilitas stroke=1 untuk setiap intervensi
prob_stroke_1 = [intervention_results[intervention][1] for intervention in labels]

# Grafik batang
x = np.arange(len(labels))  # Posisi untuk label intervensi
width = 0.4

plt.figure(figsize=(10, 6))

# Plot baseline (tanpa intervensi)
plt.bar(x - width / 2, [baseline[1]] * len(labels), width, label="Baseline (P(stroke=1))", color="gray")

# Plot hasil intervensi
plt.bar(x + width / 2, prob_stroke_1, width, label="After Intervention (P(stroke=1))", color="skyblue")

# Tambahkan label dan judul
plt.ylabel("Probability of Stroke (P(stroke=1))")
plt.xlabel("Intervention")
plt.title("Effect of Interventions and Combination on Stroke Probability")
plt.xticks(x, labels, rotation=45)
plt.legend()

# Tampilkan grafik
plt.tight_layout()
plt.show()

# Kombinasi intervensi untuk semua variabel
combination_intervention = inference.query(
    variables=["stroke"],
    evidence={
        "age": 2,                # Kategori tertinggi untuk age
        "avg_glucose_level": 2,  # Kategori tertinggi untuk avg_glucose_level
        "hypertension": 1,       # Memiliki hipertensi
        "heart_disease": 1,      # Memiliki penyakit jantung
        "ever_married_yes": 1    # Pernah menikah
    }
)
print("\nIntervention: P(stroke | age=2, avg_glucose_level=2, hypertension=1, heart_disease=1, ever_married_yes=1):")
print(combination_intervention)

import matplotlib.pyplot as plt
import numpy as np

# Data baseline dan hasil intervensi
baseline = [0.9542, 0.0458]  # P(stroke=0), P(stroke=1)
intervention_results = {
    "age=2": [0.8832, 0.1168],
    "avg_glucose_level=2": [0.9100, 0.0900],  # Contoh hasil, bisa disesuaikan
    "hypertension=1": [0.9140, 0.0860],
    "heart_disease=1": [0.8984, 0.1016],
    "ever_married_yes=1": [0.9200, 0.0800],  # Contoh hasil, bisa disesuaikan
    "All Combined": [0.8832, 0.1168],  # Kombinasi semua variabel
}

# Label intervensi
labels = list(intervention_results.keys())

# Probabilitas stroke=1 untuk setiap intervensi
prob_stroke_1 = [intervention_results[intervention][1] for intervention in labels]

# Grafik batang
x = np.arange(len(labels))  # Posisi untuk label intervensi
width = 0.4

plt.figure(figsize=(12, 6))

# Plot baseline (tanpa intervensi)
plt.bar(x - width / 2, [baseline[1]] * len(labels), width, label="Baseline (P(stroke=1))", color="gray")

# Plot hasil intervensi
plt.bar(x + width / 2, prob_stroke_1, width, label="After Intervention (P(stroke=1))", color="skyblue")

# Tambahkan label dan judul
plt.ylabel("Probability of Stroke (P(stroke=1))")
plt.xlabel("Intervention")
plt.title("Effect of Interventions on Stroke Probability")
plt.xticks(x, labels, rotation=45, ha="right")
plt.legend()

# Tampilkan grafik
plt.tight_layout()
plt.show()

# Import library
import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import HillClimbSearch, BicScore
from pgmpy.inference import VariableElimination
from sklearn.model_selection import train_test_split

# Load dataset
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/stroke_cleaned_for_model.csv")

# Fokus pada fitur yang relevan
selected_features = ["age", "avg_glucose_level", "hypertension", "heart_disease", "ever_married_yes", "stroke"]
data = data[selected_features]

# Imputasi nilai hilang (jika ada)
data.fillna(data.median(), inplace=True)

# Diskritisasi data numerik (Bayesian Network membutuhkan data diskret)
for col in ["age", "avg_glucose_level"]:
    data[col] = pd.qcut(data[col], q=3, labels=False)  # Diskritisasi menjadi 3 kategori

# Split dataset menjadi train dan test
train, test = train_test_split(data, test_size=0.2, random_state=42)

# 1. Struktur Bayesian Network menggunakan HillClimbSearch
hc = HillClimbSearch(train)
model_structure = hc.estimate(scoring_method=BicScore(train))

print("Edges in Bayesian Network:", model_structure.edges())

# 2. Definisikan model Bayesian Network berdasarkan struktur yang dipelajari
bn_model = BayesianNetwork(model_structure)

# 3. Fit model dengan data
bn_model.fit(train)

# 4. Inferensi menggunakan Variable Elimination
inference = VariableElimination(bn_model)

# Probabilitas stroke dengan mempertimbangkan semua variabel
# Contoh input:
evidence = {
    "age": 2,                # Usia dalam kategori tertinggi
    "avg_glucose_level": 2,  # Kadar glukosa rata-rata kategori tertinggi
    "hypertension": 1,       # Memiliki hipertensi
    "heart_disease": 1,      # Memiliki penyakit jantung
    "ever_married_yes": 1    # Pernah menikah
}

# Hitung probabilitas stroke
stroke_probabilities = inference.query(variables=["stroke"], evidence=evidence)
print("\nProbability of Stroke with Evidence:")
print(stroke_probabilities)